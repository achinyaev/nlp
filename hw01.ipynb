{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Осуществим предобработку данных с Твиттера, чтобы отчищенный данные в дальнейшем использовать для задачи классификации. Данный датасет содержит негативные (label = 1) и нейтральные (label = 0) высказывания.\n",
    "Для работы объединим train_df и test_df.\n",
    "\n",
    "Задания:\n",
    "\n",
    "1) Удалим @user из всех твитов с помощью паттерна \"@[\\w]*\". Для этого создадим функцию: \n",
    " - для того, чтобы найти все вхождения паттерна в тексте, необходимо использовать re.findall(pattern, input_txt)\n",
    " - для для замены @user на пробел, необходимо использовать re.sub()\n",
    "\n",
    "2) Изменим регистр твитов на нижний с помощью .lower().\n",
    "\n",
    "3) Заменим сокращения с апострофами (пример: ain't, can't) на пробел, используя apostrophe_dict. Для этого необходимо сделать функцию: для каждого слова в тексте проверить (for word in text.split()), если слово есть в словаре apostrophe_dict в качестве ключа (сокращенного слова), то заменить ключ на значение (полную версию слова).\n",
    "\n",
    "4) Заменим сокращения на их полные формы, используя short_word_dict. Для этого воспользуемся функцией, используемой в предыдущем пункте.\n",
    "\n",
    "5) Заменим эмотиконы (пример: \":)\" = \"happy\") на пробелы, используя emoticon_dict. Для этого воспользуемся функцией, используемой в предыдущем пункте.\n",
    "\n",
    "6) Заменим пунктуацию на пробелы, используя re.sub() и паттерн r'[^\\w\\s]'.\n",
    "\n",
    "7) Заменим спец. символы на пробелы, используя re.sub() и паттерн r'[^a-zA-Z0-9]'.\n",
    "\n",
    "8) Заменим числа на пробелы, используя re.sub() и паттерн r'[^a-zA-Z]'.\n",
    "\n",
    "9) Удалим из текста слова длиной в 1 символ, используя ' '.join([w for w in x.split() if len(w)>1]).\n",
    "\n",
    "10) Поделим твиты на токены с помощью nltk.tokenize.word_tokenize, создав новый столбец 'tweet_token'.\n",
    "\n",
    "11) Удалим стоп-слова из токенов, используя nltk.corpus.stopwords. Создадим столбец 'tweet_token_filtered' без стоп-слов.\n",
    "\n",
    "12) Применим стемминг к токенам с помощью nltk.stem.PorterStemmer. Создадим столбец 'tweet_stemmed' после применения стемминга.\n",
    "\n",
    "13) Применим лемматизацию к токенам с помощью nltk.stem.wordnet.WordNetLemmatizer. Создадим столбец 'tweet_lemmatized' после применения лемматизации.\n",
    "\n",
    "14) Сохраним результат предобработки в pickle-файл."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "apostrophe_dict = {\n",
    "\"ain't\": \"am not / are not\",\n",
    "\"aren't\": \"are not / am not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he had / he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he shall / he will\",\n",
    "\"he'll've\": \"he shall have / he will have\",\n",
    "\"he's\": \"he has / he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how has / how is\",\n",
    "\"i'd\": \"I had / I would\",\n",
    "\"i'd've\": \"I would have\",\n",
    "\"i'll\": \"I shall / I will\",\n",
    "\"i'll've\": \"I shall have / I will have\",\n",
    "\"i'm\": \"I am\",\n",
    "\"i've\": \"I have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it had / it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it shall / it will\",\n",
    "\"it'll've\": \"it shall have / it will have\",\n",
    "\"it's\": \"it has / it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she had / she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she shall / she will\",\n",
    "\"she'll've\": \"she shall have / she will have\",\n",
    "\"she's\": \"she has / she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so as / so is\",\n",
    "\"that'd\": \"that would / that had\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that has / that is\",\n",
    "\"there'd\": \"there had / there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there has / there is\",\n",
    "\"they'd\": \"they had / they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they shall / they will\",\n",
    "\"they'll've\": \"they shall have / they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we had / we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what shall / what will\",\n",
    "\"what'll've\": \"what shall have / what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what has / what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when has / when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where has / where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who shall / who will\",\n",
    "\"who'll've\": \"who shall have / who will have\",\n",
    "\"who's\": \"who has / who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why has / why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you had / you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you shall / you will\",\n",
    "\"you'll've\": \"you shall have / you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "short_word_dict = {\n",
    "\"121\": \"one to one\",\n",
    "\"a/s/l\": \"age, sex, location\",\n",
    "\"adn\": \"any day now\",\n",
    "\"afaik\": \"as far as I know\",\n",
    "\"afk\": \"away from keyboard\",\n",
    "\"aight\": \"alright\",\n",
    "\"alol\": \"actually laughing out loud\",\n",
    "\"b4\": \"before\",\n",
    "\"b4n\": \"bye for now\",\n",
    "\"bak\": \"back at the keyboard\",\n",
    "\"bf\": \"boyfriend\",\n",
    "\"bff\": \"best friends forever\",\n",
    "\"bfn\": \"bye for now\",\n",
    "\"bg\": \"big grin\",\n",
    "\"bta\": \"but then again\",\n",
    "\"btw\": \"by the way\",\n",
    "\"cid\": \"crying in disgrace\",\n",
    "\"cnp\": \"continued in my next post\",\n",
    "\"cp\": \"chat post\",\n",
    "\"cu\": \"see you\",\n",
    "\"cul\": \"see you later\",\n",
    "\"cul8r\": \"see you later\",\n",
    "\"cya\": \"bye\",\n",
    "\"cyo\": \"see you online\",\n",
    "\"dbau\": \"doing business as usual\",\n",
    "\"fud\": \"fear, uncertainty, and doubt\",\n",
    "\"fwiw\": \"for what it's worth\",\n",
    "\"fyi\": \"for your information\",\n",
    "\"g\": \"grin\",\n",
    "\"g2g\": \"got to go\",\n",
    "\"ga\": \"go ahead\",\n",
    "\"gal\": \"get a life\",\n",
    "\"gf\": \"girlfriend\",\n",
    "\"gfn\": \"gone for now\",\n",
    "\"gmbo\": \"giggling my butt off\",\n",
    "\"gmta\": \"great minds think alike\",\n",
    "\"h8\": \"hate\",\n",
    "\"hagn\": \"have a good night\",\n",
    "\"hdop\": \"help delete online predators\",\n",
    "\"hhis\": \"hanging head in shame\",\n",
    "\"iac\": \"in any case\",\n",
    "\"ianal\": \"I am not a lawyer\",\n",
    "\"ic\": \"I see\",\n",
    "\"idk\": \"I don't know\",\n",
    "\"imao\": \"in my arrogant opinion\",\n",
    "\"imnsho\": \"in my not so humble opinion\",\n",
    "\"imo\": \"in my opinion\",\n",
    "\"iow\": \"in other words\",\n",
    "\"ipn\": \"I’m posting naked\",\n",
    "\"irl\": \"in real life\",\n",
    "\"jk\": \"just kidding\",\n",
    "\"l8r\": \"later\",\n",
    "\"ld\": \"later, dude\",\n",
    "\"ldr\": \"long distance relationship\",\n",
    "\"llta\": \"lots and lots of thunderous applause\",\n",
    "\"lmao\": \"laugh my ass off\",\n",
    "\"lmirl\": \"let's meet in real life\",\n",
    "\"lol\": \"laugh out loud\",\n",
    "\"ltr\": \"longterm relationship\",\n",
    "\"lulab\": \"love you like a brother\",\n",
    "\"lulas\": \"love you like a sister\",\n",
    "\"luv\": \"love\",\n",
    "\"m/f\": \"male or female\",\n",
    "\"m8\": \"mate\",\n",
    "\"milf\": \"mother I would like to fuck\",\n",
    "\"oll\": \"online love\",\n",
    "\"omg\": \"oh my god\",\n",
    "\"otoh\": \"on the other hand\",\n",
    "\"pir\": \"parent in room\",\n",
    "\"ppl\": \"people\",\n",
    "\"r\": \"are\",\n",
    "\"rofl\": \"roll on the floor laughing\",\n",
    "\"rpg\": \"role playing games\",\n",
    "\"ru\": \"are you\",\n",
    "\"shid\": \"slaps head in disgust\",\n",
    "\"somy\": \"sick of me yet\",\n",
    "\"sot\": \"short of time\",\n",
    "\"thanx\": \"thanks\",\n",
    "\"thx\": \"thanks\",\n",
    "\"ttyl\": \"talk to you later\",\n",
    "\"u\": \"you\",\n",
    "\"ur\": \"you are\",\n",
    "\"uw\": \"you’re welcome\",\n",
    "\"wb\": \"welcome back\",\n",
    "\"wfm\": \"works for me\",\n",
    "\"wibni\": \"wouldn't it be nice if\",\n",
    "\"wtf\": \"what the fuck\",\n",
    "\"wtg\": \"way to go\",\n",
    "\"wtgp\": \"want to go private\",\n",
    "\"ym\": \"young man\",\n",
    "\"gr8\": \"great\"\n",
    "}\n",
    "\n",
    "\n",
    "emoticon_dict = {\n",
    "\":)\": \"happy\",\n",
    "\":‑)\": \"happy\",\n",
    "\":-]\": \"happy\",\n",
    "\":-3\": \"happy\",\n",
    "\":->\": \"happy\",\n",
    "\"8-)\": \"happy\",\n",
    "\":-}\": \"happy\",\n",
    "\":o)\": \"happy\",\n",
    "\":c)\": \"happy\",\n",
    "\":^)\": \"happy\",\n",
    "\"=]\": \"happy\",\n",
    "\"=)\": \"happy\",\n",
    "\"<3\": \"happy\",\n",
    "\":-(\": \"sad\",\n",
    "\":(\": \"sad\",\n",
    "\":c\": \"sad\",\n",
    "\":<\": \"sad\",\n",
    "\":[\": \"sad\",\n",
    "\">:[\": \"sad\",\n",
    "\":{\": \"sad\",\n",
    "\">:(\": \"sad\",\n",
    "\":-c\": \"sad\",\n",
    "\":-< \": \"sad\",\n",
    "\":-[\": \"sad\",\n",
    "\":-||\": \"sad\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet\n",
       "0   1      0   @user when a father is dysfunctional and is s...\n",
       "1   2      0  @user @user thanks for #lyft credit i can't us...\n",
       "2   3      0                                bihday your majesty\n",
       "3   4      0  #model   i love u take with u all the time in ...\n",
       "4   5      0             factsguide: society now    #motivation"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('train_tweets.csv')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31963</td>\n",
       "      <td>#studiolife #aislife #requires #passion #dedic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31964</td>\n",
       "      <td>@user #white #supremacists want everyone to s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31965</td>\n",
       "      <td>safe ways to heal your #acne!!    #altwaystohe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31966</td>\n",
       "      <td>is the hp and the cursed child book up for res...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31967</td>\n",
       "      <td>3rd #bihday to my amazing, hilarious #nephew...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                              tweet\n",
       "0  31963  #studiolife #aislife #requires #passion #dedic...\n",
       "1  31964   @user #white #supremacists want everyone to s...\n",
       "2  31965  safe ways to heal your #acne!!    #altwaystohe...\n",
       "3  31966  is the hp and the cursed child book up for res...\n",
       "4  31967    3rd #bihday to my amazing, hilarious #nephew..."
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv('test_tweets.csv')\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet\n",
       "0   1    0.0   @user when a father is dysfunctional and is s...\n",
       "1   2    0.0  @user @user thanks for #lyft credit i can't us...\n",
       "2   3    0.0                                bihday your majesty\n",
       "3   4    0.0  #model   i love u take with u all the time in ...\n",
       "4   5    0.0             factsguide: society now    #motivation"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combine_df = train_df.append(test_df, ignore_index = True, sort = False)\n",
    "combine_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 49159 entries, 0 to 49158\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   id      49159 non-null  int64  \n",
      " 1   label   31962 non-null  float64\n",
      " 2   tweet   49159 non-null  object \n",
      "dtypes: float64(1), int64(1), object(1)\n",
      "memory usage: 1.1+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(combine_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Удалим @user из всех твитов с помощью паттерна \"@[\\w]*\". Для этого создадим функцию: \n",
    " - для того, чтобы найти все вхождения паттерна в тексте, необходимо использовать re.findall(pattern, input_txt)\n",
    " - для для замены @user на пробел, необходимо использовать re.sub()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Изменим регистр твитов на нижний с помощью .lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Заменим сокращения с апострофами (пример: ain't, can't) на пробел, используя apostrophe_dict. Для этого необходимо сделать функцию: для каждого слова в тексте проверить (for word in text.split()), если слово есть в словаре apostrophe_dict в качестве ключа (сокращенного слова), то заменить ключ на значение (полную версию слова)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Заменим сокращения на их полные формы, используя short_word_dict. Для этого воспользуемся функцией, используемой в предыдущем пункте."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Заменим эмотиконы (пример: \":)\" = \"happy\") на пробелы, используя emoticon_dict. Для этого воспользуемся функцией, используемой в предыдущем пункте."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Заменим пунктуацию на пробелы, используя re.sub() и паттерн r'[^\\w\\s]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Заменим спец. символы на пробелы, используя re.sub() и паттерн r'[^a-zA-Z0-9]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Заменим числа на пробелы, используя re.sub() и паттерн r'[^a-zA-Z]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Удалим из текста слова длиной в 1 символ, используя ' '.join([w for w in x.split() if len(w)>1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Поделим твиты на токены с помощью nltk.tokenize.word_tokenize, создав новый столбец 'tweet_token'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Удалим стоп-слова из токенов, используя nltk.corpus.stopwords. Создадим столбец 'tweet_token_filtered' без стоп-слов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. Применим стемминг к токенам с помощью nltk.stem.PorterStemmer. Создадим столбец 'tweet_stemmed' после применения стемминга."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. Применим лемматизацию к токенам с помощью nltk.stem.wordnet.WordNetLemmatizer. Создадим столбец 'tweet_lemmatized' после применения лемматизации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14. Сохраним результат предобработки в pickle-файл."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         @user when a father is dysfunctional and is s...\n",
       "1        @user @user thanks for #lyft credit i can't us...\n",
       "2                                      bihday your majesty\n",
       "3        #model   i love u take with u all the time in ...\n",
       "4                   factsguide: society now    #motivation\n",
       "                               ...                        \n",
       "49154    thought factory: left-right polarisation! #tru...\n",
       "49155    feeling like a mermaid ð #hairflip #neverre...\n",
       "49156    #hillary #campaigned today in #ohio((omg)) &am...\n",
       "49157    happy, at work conference: right mindset leads...\n",
       "49158    my   song \"so glad\" free download!  #shoegaze ...\n",
       "Name: tweet, Length: 49159, dtype: object"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combine_df['tweet']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Удалим @user из всех твитов с помощью паттерна \"@[\\w]*\". Для этого создадим функцию:\n",
    "для того, чтобы найти все вхождения паттерна в тексте, необходимо использовать re.findall(pattern, input_txt)<br>\n",
    "для для замены @user на пробел, необходимо использовать re.sub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_regex(pattern: str, pattern2:str , data: pd.DataFrame):\n",
    "    data['tweet'] = list(map(lambda x: re.sub(pattern, pattern2, x), data['tweet'].values))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_regex('@[\\w]*',' ',combine_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>when a father is dysfunctional and is so se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>thanks for #lyft credit i can't use cause ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet\n",
       "0   1    0.0     when a father is dysfunctional and is so se...\n",
       "1   2    0.0      thanks for #lyft credit i can't use cause ...\n",
       "2   3    0.0                                bihday your majesty\n",
       "3   4    0.0  #model   i love u take with u all the time in ...\n",
       "4   5    0.0             factsguide: society now    #motivation"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combine_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Изменим регистр твитов на нижний с помощью .lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_df['tweet'] = list(map(str.lower, combine_df['tweet'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>when a father is dysfunctional and is so se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>thanks for #lyft credit i can't use cause ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet\n",
       "0   1    0.0     when a father is dysfunctional and is so se...\n",
       "1   2    0.0      thanks for #lyft credit i can't use cause ...\n",
       "2   3    0.0                                bihday your majesty\n",
       "3   4    0.0  #model   i love u take with u all the time in ...\n",
       "4   5    0.0             factsguide: society now    #motivation"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combine_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_replace(txt:str , dic:dict):\n",
    "    result = \"\"\n",
    "    for word in txt.split():\n",
    "        if word in dic.keys():\n",
    "            result += dic[word]\n",
    "        else:\n",
    "            result += word\n",
    "        result += \" \"\n",
    "    return result[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Заменим сокращения с апострофами (пример: ain't, can't) на пробел, используя apostrophe_dict. Для этого необходимо сделать функцию: для каждого слова в тексте проверить (for word in text.split()), если слово есть в словаре apostrophe_dict в качестве ключа (сокращенного слова), то заменить ключ на значение (полную версию слова)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>when a father is dysfunctional and is so selfi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>thanks for #lyft credit i cannot use cause the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>#model i love u take with u all the time in ur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>factsguide: society now #motivation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet\n",
       "0   1    0.0  when a father is dysfunctional and is so selfi...\n",
       "1   2    0.0  thanks for #lyft credit i cannot use cause the...\n",
       "2   3    0.0                                bihday your majesty\n",
       "3   4    0.0  #model i love u take with u all the time in ur...\n",
       "4   5    0.0                factsguide: society now #motivation"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combine_df['tweet'] = list(map(lambda x: dict_replace(x,apostrophe_dict), combine_df['tweet'].values))\n",
    "combine_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Заменим сокращения на их полные формы, используя short_word_dict. Для этого воспользуемся функцией, используемой в предыдущем пункте."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>when a father is dysfunctional and is so selfi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>thanks for #lyft credit i cannot use cause the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>#model i love you take with you all the time i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>factsguide: society now #motivation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet\n",
       "0   1    0.0  when a father is dysfunctional and is so selfi...\n",
       "1   2    0.0  thanks for #lyft credit i cannot use cause the...\n",
       "2   3    0.0                                bihday your majesty\n",
       "3   4    0.0  #model i love you take with you all the time i...\n",
       "4   5    0.0                factsguide: society now #motivation"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combine_df['tweet'] = list(map(lambda x: dict_replace(x,short_word_dict), combine_df['tweet'].values))\n",
    "combine_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Заменим эмотиконы (пример: \":)\" = \"happy\") на пробелы, используя emoticon_dict. Для этого воспользуемся функцией, используемой в предыдущем пункте."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>when a father is dysfunctional and is so selfi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>thanks for #lyft credit i cannot use cause the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>#model i love you take with you all the time i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>factsguide: society now #motivation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet\n",
       "0   1    0.0  when a father is dysfunctional and is so selfi...\n",
       "1   2    0.0  thanks for #lyft credit i cannot use cause the...\n",
       "2   3    0.0                                bihday your majesty\n",
       "3   4    0.0  #model i love you take with you all the time i...\n",
       "4   5    0.0                factsguide: society now #motivation"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combine_df['tweet'] = list(map(lambda x: dict_replace(x,emoticon_dict), combine_df['tweet'].values))\n",
    "combine_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Заменим пунктуацию на пробелы, используя re.sub() и паттерн r'[^\\w\\s]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_regex('[^\\w\\s]',' ',combine_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        when a father is dysfunctional and is so selfi...\n",
      "1        thanks for  lyft credit i cannot use cause the...\n",
      "2                                      bihday your majesty\n",
      "3         model i love you take with you all the time i...\n",
      "4                      factsguide  society now  motivation\n",
      "                               ...                        \n",
      "49154    thought factory  left right polarisation   tru...\n",
      "49155    feeling like a mermaid ð     hairflip  neverre...\n",
      "49156     hillary  campaigned today in  ohio  omg    am...\n",
      "49157    happy  at work conference  right mindset leads...\n",
      "49158    my song  so glad  free download   shoegaze  ne...\n",
      "Name: tweet, Length: 49159, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(combine_df['tweet'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Заменим спец. символы на пробелы, используя re.sub() и паттерн r'[^a-zA-Z0-9]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_regex('[^a-zA-Z0-9]',' ',combine_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        when a father is dysfunctional and is so selfi...\n",
      "1        thanks for  lyft credit i cannot use cause the...\n",
      "2                                      bihday your majesty\n",
      "3         model i love you take with you all the time i...\n",
      "4                      factsguide  society now  motivation\n",
      "                               ...                        \n",
      "49154    thought factory  left right polarisation   tru...\n",
      "49155    feeling like a mermaid       hairflip  neverre...\n",
      "49156     hillary  campaigned today in  ohio  omg    am...\n",
      "49157    happy  at work conference  right mindset leads...\n",
      "49158    my song  so glad  free download   shoegaze  ne...\n",
      "Name: tweet, Length: 49159, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(combine_df['tweet'].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Заменим числа на пробелы, используя re.sub() и паттерн r'[^a-zA-Z]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    when a father is dysfunctional and is so selfi...\n",
      "1    thanks for  lyft credit i cannot use cause the...\n",
      "2                                  bihday your majesty\n",
      "3     model i love you take with you all the time i...\n",
      "4                  factsguide  society now  motivation\n",
      "5          huge fan fare and big talking before the...\n",
      "6                            camping tomorrow danny   \n",
      "7    the next school year is the year for exams    ...\n",
      "8    we won    love the land     allin  cavs  champ...\n",
      "9         welcome here   I am it has   it is so  gr   \n",
      "Name: tweet, dtype: object\n"
     ]
    }
   ],
   "source": [
    "replace_regex('[^a-zA-Z]',' ',combine_df)\n",
    "print(combine_df['tweet'].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Удалим из текста слова длиной в 1 символ, используя ' '.join([w for w in x.split() if len(w)>1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    when father is dysfunctional and is so selfish...\n",
       "1    thanks for lyft credit cannot use cause they d...\n",
       "2                                  bihday your majesty\n",
       "3      model love you take with you all the time in ur\n",
       "4                    factsguide society now motivation\n",
       "5    huge fan fare and big talking before they leav...\n",
       "6                               camping tomorrow danny\n",
       "7    the next school year is the year for exams can...\n",
       "8    we won love the land allin cavs champions clev...\n",
       "9                   welcome here am it has it is so gr\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combine_df[\"tweet\"] = list(map(lambda x: ' '.join([w for w in x.split() if len(w)>1]), combine_df[\"tweet\"].values))\n",
    "combine_df['tweet'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Поделим твиты на токены с помощью nltk.tokenize.word_tokenize, создав новый столбец 'tweet_token'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tweet_token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49149</th>\n",
       "      <td>49150</td>\n",
       "      <td>NaN</td>\n",
       "      <td>loving life createyourfuture lifestyle holiday...</td>\n",
       "      <td>[loving, life, createyourfuture, lifestyle, ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49150</th>\n",
       "      <td>49151</td>\n",
       "      <td>NaN</td>\n",
       "      <td>black professor demonizes proposes nazi style ...</td>\n",
       "      <td>[black, professor, demonizes, proposes, nazi, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49151</th>\n",
       "      <td>49152</td>\n",
       "      <td>NaN</td>\n",
       "      <td>learn how to think positive positive instagram...</td>\n",
       "      <td>[learn, how, to, think, positive, positive, in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49152</th>\n",
       "      <td>49153</td>\n",
       "      <td>NaN</td>\n",
       "      <td>we love the pretty happy and fresh you teenili...</td>\n",
       "      <td>[we, love, the, pretty, happy, and, fresh, you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49153</th>\n",
       "      <td>49154</td>\n",
       "      <td>NaN</td>\n",
       "      <td>damn tuff ruff muff techno city ng web ukhx in...</td>\n",
       "      <td>[damn, tuff, ruff, muff, techno, city, ng, web...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49154</th>\n",
       "      <td>49155</td>\n",
       "      <td>NaN</td>\n",
       "      <td>thought factory left right polarisation trump ...</td>\n",
       "      <td>[thought, factory, left, right, polarisation, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49155</th>\n",
       "      <td>49156</td>\n",
       "      <td>NaN</td>\n",
       "      <td>feeling like mermaid hairflip neverready forma...</td>\n",
       "      <td>[feeling, like, mermaid, hairflip, neverready,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49156</th>\n",
       "      <td>49157</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hillary campaigned today in ohio omg amp used ...</td>\n",
       "      <td>[hillary, campaigned, today, in, ohio, omg, am...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49157</th>\n",
       "      <td>49158</td>\n",
       "      <td>NaN</td>\n",
       "      <td>happy at work conference right mindset leads t...</td>\n",
       "      <td>[happy, at, work, conference, right, mindset, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49158</th>\n",
       "      <td>49159</td>\n",
       "      <td>NaN</td>\n",
       "      <td>my song so glad free download shoegaze newmusi...</td>\n",
       "      <td>[my, song, so, glad, free, download, shoegaze,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  label                                              tweet  \\\n",
       "49149  49150    NaN  loving life createyourfuture lifestyle holiday...   \n",
       "49150  49151    NaN  black professor demonizes proposes nazi style ...   \n",
       "49151  49152    NaN  learn how to think positive positive instagram...   \n",
       "49152  49153    NaN  we love the pretty happy and fresh you teenili...   \n",
       "49153  49154    NaN  damn tuff ruff muff techno city ng web ukhx in...   \n",
       "49154  49155    NaN  thought factory left right polarisation trump ...   \n",
       "49155  49156    NaN  feeling like mermaid hairflip neverready forma...   \n",
       "49156  49157    NaN  hillary campaigned today in ohio omg amp used ...   \n",
       "49157  49158    NaN  happy at work conference right mindset leads t...   \n",
       "49158  49159    NaN  my song so glad free download shoegaze newmusi...   \n",
       "\n",
       "                                             tweet_token  \n",
       "49149  [loving, life, createyourfuture, lifestyle, ho...  \n",
       "49150  [black, professor, demonizes, proposes, nazi, ...  \n",
       "49151  [learn, how, to, think, positive, positive, in...  \n",
       "49152  [we, love, the, pretty, happy, and, fresh, you...  \n",
       "49153  [damn, tuff, ruff, muff, techno, city, ng, web...  \n",
       "49154  [thought, factory, left, right, polarisation, ...  \n",
       "49155  [feeling, like, mermaid, hairflip, neverready,...  \n",
       "49156  [hillary, campaigned, today, in, ohio, omg, am...  \n",
       "49157  [happy, at, work, conference, right, mindset, ...  \n",
       "49158  [my, song, so, glad, free, download, shoegaze,...  "
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combine_df['tweet_token'] = list(map(word_tokenize, combine_df[\"tweet\"].values))\n",
    "combine_df.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Удалим стоп-слова из токенов, используя nltk.corpus.stopwords. Создадим столбец 'tweet_token_filtered' без стоп-слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\chinyaev.av\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "print(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_df['tweet_token_filtered'] = list(map(lambda x: [word for word in x if not word in stopwords.words(\"english\")], combine_df[\"tweet_token\"].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tweet_token</th>\n",
       "      <th>tweet_token_filtered</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49149</th>\n",
       "      <td>49150</td>\n",
       "      <td>NaN</td>\n",
       "      <td>loving life createyourfuture lifestyle holiday...</td>\n",
       "      <td>[loving, life, createyourfuture, lifestyle, ho...</td>\n",
       "      <td>[loving, life, createyourfuture, lifestyle, ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49150</th>\n",
       "      <td>49151</td>\n",
       "      <td>NaN</td>\n",
       "      <td>black professor demonizes proposes nazi style ...</td>\n",
       "      <td>[black, professor, demonizes, proposes, nazi, ...</td>\n",
       "      <td>[black, professor, demonizes, proposes, nazi, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49151</th>\n",
       "      <td>49152</td>\n",
       "      <td>NaN</td>\n",
       "      <td>learn how to think positive positive instagram...</td>\n",
       "      <td>[learn, how, to, think, positive, positive, in...</td>\n",
       "      <td>[learn, think, positive, positive, instagram, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49152</th>\n",
       "      <td>49153</td>\n",
       "      <td>NaN</td>\n",
       "      <td>we love the pretty happy and fresh you teenili...</td>\n",
       "      <td>[we, love, the, pretty, happy, and, fresh, you...</td>\n",
       "      <td>[love, pretty, happy, fresh, teenilicious, fix...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49153</th>\n",
       "      <td>49154</td>\n",
       "      <td>NaN</td>\n",
       "      <td>damn tuff ruff muff techno city ng web ukhx in...</td>\n",
       "      <td>[damn, tuff, ruff, muff, techno, city, ng, web...</td>\n",
       "      <td>[damn, tuff, ruff, muff, techno, city, ng, web...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49154</th>\n",
       "      <td>49155</td>\n",
       "      <td>NaN</td>\n",
       "      <td>thought factory left right polarisation trump ...</td>\n",
       "      <td>[thought, factory, left, right, polarisation, ...</td>\n",
       "      <td>[thought, factory, left, right, polarisation, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49155</th>\n",
       "      <td>49156</td>\n",
       "      <td>NaN</td>\n",
       "      <td>feeling like mermaid hairflip neverready forma...</td>\n",
       "      <td>[feeling, like, mermaid, hairflip, neverready,...</td>\n",
       "      <td>[feeling, like, mermaid, hairflip, neverready,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49156</th>\n",
       "      <td>49157</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hillary campaigned today in ohio omg amp used ...</td>\n",
       "      <td>[hillary, campaigned, today, in, ohio, omg, am...</td>\n",
       "      <td>[hillary, campaigned, today, ohio, omg, amp, u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49157</th>\n",
       "      <td>49158</td>\n",
       "      <td>NaN</td>\n",
       "      <td>happy at work conference right mindset leads t...</td>\n",
       "      <td>[happy, at, work, conference, right, mindset, ...</td>\n",
       "      <td>[happy, work, conference, right, mindset, lead...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49158</th>\n",
       "      <td>49159</td>\n",
       "      <td>NaN</td>\n",
       "      <td>my song so glad free download shoegaze newmusi...</td>\n",
       "      <td>[my, song, so, glad, free, download, shoegaze,...</td>\n",
       "      <td>[song, glad, free, download, shoegaze, newmusi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  label                                              tweet  \\\n",
       "49149  49150    NaN  loving life createyourfuture lifestyle holiday...   \n",
       "49150  49151    NaN  black professor demonizes proposes nazi style ...   \n",
       "49151  49152    NaN  learn how to think positive positive instagram...   \n",
       "49152  49153    NaN  we love the pretty happy and fresh you teenili...   \n",
       "49153  49154    NaN  damn tuff ruff muff techno city ng web ukhx in...   \n",
       "49154  49155    NaN  thought factory left right polarisation trump ...   \n",
       "49155  49156    NaN  feeling like mermaid hairflip neverready forma...   \n",
       "49156  49157    NaN  hillary campaigned today in ohio omg amp used ...   \n",
       "49157  49158    NaN  happy at work conference right mindset leads t...   \n",
       "49158  49159    NaN  my song so glad free download shoegaze newmusi...   \n",
       "\n",
       "                                             tweet_token  \\\n",
       "49149  [loving, life, createyourfuture, lifestyle, ho...   \n",
       "49150  [black, professor, demonizes, proposes, nazi, ...   \n",
       "49151  [learn, how, to, think, positive, positive, in...   \n",
       "49152  [we, love, the, pretty, happy, and, fresh, you...   \n",
       "49153  [damn, tuff, ruff, muff, techno, city, ng, web...   \n",
       "49154  [thought, factory, left, right, polarisation, ...   \n",
       "49155  [feeling, like, mermaid, hairflip, neverready,...   \n",
       "49156  [hillary, campaigned, today, in, ohio, omg, am...   \n",
       "49157  [happy, at, work, conference, right, mindset, ...   \n",
       "49158  [my, song, so, glad, free, download, shoegaze,...   \n",
       "\n",
       "                                    tweet_token_filtered  \n",
       "49149  [loving, life, createyourfuture, lifestyle, ho...  \n",
       "49150  [black, professor, demonizes, proposes, nazi, ...  \n",
       "49151  [learn, think, positive, positive, instagram, ...  \n",
       "49152  [love, pretty, happy, fresh, teenilicious, fix...  \n",
       "49153  [damn, tuff, ruff, muff, techno, city, ng, web...  \n",
       "49154  [thought, factory, left, right, polarisation, ...  \n",
       "49155  [feeling, like, mermaid, hairflip, neverready,...  \n",
       "49156  [hillary, campaigned, today, ohio, omg, amp, u...  \n",
       "49157  [happy, work, conference, right, mindset, lead...  \n",
       "49158  [song, glad, free, download, shoegaze, newmusi...  "
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combine_df.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Применим стемминг к токенам с помощью nltk.stem.PorterStemmer. Создадим столбец 'tweet_stemmed' после применения стемминга."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['father', 'dysfunct', 'selfish', 'drag', 'kid', 'dysfunct', 'run']"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[stemmer.stem(word) for word in combine_df[\"tweet_token_filtered\"].values[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "combine_df['tweet_stemmed'] = list(map(lambda x: set(stemmer.stem(word) for word in x), combine_df[\"tweet_token_filtered\"].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tweet_token</th>\n",
       "      <th>tweet_token_filtered</th>\n",
       "      <th>tweet_stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49149</th>\n",
       "      <td>49150</td>\n",
       "      <td>NaN</td>\n",
       "      <td>loving life createyourfuture lifestyle holiday...</td>\n",
       "      <td>[loving, life, createyourfuture, lifestyle, ho...</td>\n",
       "      <td>[loving, life, createyourfuture, lifestyle, ho...</td>\n",
       "      <td>{life, beach, love, holiday, hyatt, lifestyl, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49150</th>\n",
       "      <td>49151</td>\n",
       "      <td>NaN</td>\n",
       "      <td>black professor demonizes proposes nazi style ...</td>\n",
       "      <td>[black, professor, demonizes, proposes, nazi, ...</td>\n",
       "      <td>[black, professor, demonizes, proposes, nazi, ...</td>\n",
       "      <td>{style, black, like, asset, germani, nazi, bre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49151</th>\n",
       "      <td>49152</td>\n",
       "      <td>NaN</td>\n",
       "      <td>learn how to think positive positive instagram...</td>\n",
       "      <td>[learn, how, to, think, positive, positive, in...</td>\n",
       "      <td>[learn, think, positive, positive, instagram, ...</td>\n",
       "      <td>{think, instagood, learn, instagram, posit}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49152</th>\n",
       "      <td>49153</td>\n",
       "      <td>NaN</td>\n",
       "      <td>we love the pretty happy and fresh you teenili...</td>\n",
       "      <td>[we, love, the, pretty, happy, and, fresh, you...</td>\n",
       "      <td>[love, pretty, happy, fresh, teenilicious, fix...</td>\n",
       "      <td>{fixdermateen, generationz, love, pretti, teen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49153</th>\n",
       "      <td>49154</td>\n",
       "      <td>NaN</td>\n",
       "      <td>damn tuff ruff muff techno city ng web ukhx in...</td>\n",
       "      <td>[damn, tuff, ruff, muff, techno, city, ng, web...</td>\n",
       "      <td>[damn, tuff, ruff, muff, techno, city, ng, web...</td>\n",
       "      <td>{web, vk, damn, int, ruff, tuff, gabba, citi, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49154</th>\n",
       "      <td>49155</td>\n",
       "      <td>NaN</td>\n",
       "      <td>thought factory left right polarisation trump ...</td>\n",
       "      <td>[thought, factory, left, right, polarisation, ...</td>\n",
       "      <td>[thought, factory, left, right, polarisation, ...</td>\n",
       "      <td>{thought, brexit, polaris, right, leadership, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49155</th>\n",
       "      <td>49156</td>\n",
       "      <td>NaN</td>\n",
       "      <td>feeling like mermaid hairflip neverready forma...</td>\n",
       "      <td>[feeling, like, mermaid, hairflip, neverready,...</td>\n",
       "      <td>[feeling, like, mermaid, hairflip, neverready,...</td>\n",
       "      <td>{formal, mermaid, like, feel, hairflip, gown, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49156</th>\n",
       "      <td>49157</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hillary campaigned today in ohio omg amp used ...</td>\n",
       "      <td>[hillary, campaigned, today, in, ohio, omg, am...</td>\n",
       "      <td>[hillary, campaigned, today, ohio, omg, amp, u...</td>\n",
       "      <td>{thee, today, like, clinton, asset, omg, say, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49157</th>\n",
       "      <td>49158</td>\n",
       "      <td>NaN</td>\n",
       "      <td>happy at work conference right mindset leads t...</td>\n",
       "      <td>[happy, at, work, conference, right, mindset, ...</td>\n",
       "      <td>[happy, work, conference, right, mindset, lead...</td>\n",
       "      <td>{right, lead, mindset, cultur, organ, work, co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49158</th>\n",
       "      <td>49159</td>\n",
       "      <td>NaN</td>\n",
       "      <td>my song so glad free download shoegaze newmusi...</td>\n",
       "      <td>[my, song, so, glad, free, download, shoegaze,...</td>\n",
       "      <td>[song, glad, free, download, shoegaze, newmusi...</td>\n",
       "      <td>{newsong, shoegaz, song, download, newmus, fre...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  label                                              tweet  \\\n",
       "49149  49150    NaN  loving life createyourfuture lifestyle holiday...   \n",
       "49150  49151    NaN  black professor demonizes proposes nazi style ...   \n",
       "49151  49152    NaN  learn how to think positive positive instagram...   \n",
       "49152  49153    NaN  we love the pretty happy and fresh you teenili...   \n",
       "49153  49154    NaN  damn tuff ruff muff techno city ng web ukhx in...   \n",
       "49154  49155    NaN  thought factory left right polarisation trump ...   \n",
       "49155  49156    NaN  feeling like mermaid hairflip neverready forma...   \n",
       "49156  49157    NaN  hillary campaigned today in ohio omg amp used ...   \n",
       "49157  49158    NaN  happy at work conference right mindset leads t...   \n",
       "49158  49159    NaN  my song so glad free download shoegaze newmusi...   \n",
       "\n",
       "                                             tweet_token  \\\n",
       "49149  [loving, life, createyourfuture, lifestyle, ho...   \n",
       "49150  [black, professor, demonizes, proposes, nazi, ...   \n",
       "49151  [learn, how, to, think, positive, positive, in...   \n",
       "49152  [we, love, the, pretty, happy, and, fresh, you...   \n",
       "49153  [damn, tuff, ruff, muff, techno, city, ng, web...   \n",
       "49154  [thought, factory, left, right, polarisation, ...   \n",
       "49155  [feeling, like, mermaid, hairflip, neverready,...   \n",
       "49156  [hillary, campaigned, today, in, ohio, omg, am...   \n",
       "49157  [happy, at, work, conference, right, mindset, ...   \n",
       "49158  [my, song, so, glad, free, download, shoegaze,...   \n",
       "\n",
       "                                    tweet_token_filtered  \\\n",
       "49149  [loving, life, createyourfuture, lifestyle, ho...   \n",
       "49150  [black, professor, demonizes, proposes, nazi, ...   \n",
       "49151  [learn, think, positive, positive, instagram, ...   \n",
       "49152  [love, pretty, happy, fresh, teenilicious, fix...   \n",
       "49153  [damn, tuff, ruff, muff, techno, city, ng, web...   \n",
       "49154  [thought, factory, left, right, polarisation, ...   \n",
       "49155  [feeling, like, mermaid, hairflip, neverready,...   \n",
       "49156  [hillary, campaigned, today, ohio, omg, amp, u...   \n",
       "49157  [happy, work, conference, right, mindset, lead...   \n",
       "49158  [song, glad, free, download, shoegaze, newmusi...   \n",
       "\n",
       "                                           tweet_stemmed  \n",
       "49149  {life, beach, love, holiday, hyatt, lifestyl, ...  \n",
       "49150  {style, black, like, asset, germani, nazi, bre...  \n",
       "49151        {think, instagood, learn, instagram, posit}  \n",
       "49152  {fixdermateen, generationz, love, pretti, teen...  \n",
       "49153  {web, vk, damn, int, ruff, tuff, gabba, citi, ...  \n",
       "49154  {thought, brexit, polaris, right, leadership, ...  \n",
       "49155  {formal, mermaid, like, feel, hairflip, gown, ...  \n",
       "49156  {thee, today, like, clinton, asset, omg, say, ...  \n",
       "49157  {right, lead, mindset, cultur, organ, work, co...  \n",
       "49158  {newsong, shoegaz, song, download, newmus, fre...  "
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combine_df.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Применим лемматизацию к токенам с помощью nltk.stem.wordnet.WordNetLemmatizer. Создадим столбец 'tweet_lemmatized' после применения лемматизации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\chinyaev.av\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\chinyaev.av\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\omw-1.4.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_df['tweet_lemmatized'] = list(map(lambda x: set(lemmatizer.lemmatize(word, pos=wordnet.VERB) for word in x), combine_df[\"tweet_token_filtered\"].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tweet_token</th>\n",
       "      <th>tweet_token_filtered</th>\n",
       "      <th>tweet_stemmed</th>\n",
       "      <th>tweet_lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49149</th>\n",
       "      <td>49150</td>\n",
       "      <td>NaN</td>\n",
       "      <td>loving life createyourfuture lifestyle holiday...</td>\n",
       "      <td>[loving, life, createyourfuture, lifestyle, ho...</td>\n",
       "      <td>[loving, life, createyourfuture, lifestyle, ho...</td>\n",
       "      <td>{life, beach, love, holiday, hyatt, lifestyl, ...</td>\n",
       "      <td>{createyourfuture, life, beach, love, regency,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49150</th>\n",
       "      <td>49151</td>\n",
       "      <td>NaN</td>\n",
       "      <td>black professor demonizes proposes nazi style ...</td>\n",
       "      <td>[black, professor, demonizes, proposes, nazi, ...</td>\n",
       "      <td>[black, professor, demonizes, proposes, nazi, ...</td>\n",
       "      <td>{style, black, like, asset, germani, nazi, bre...</td>\n",
       "      <td>{germany, black, like, confiscation, nazi, bre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49151</th>\n",
       "      <td>49152</td>\n",
       "      <td>NaN</td>\n",
       "      <td>learn how to think positive positive instagram...</td>\n",
       "      <td>[learn, how, to, think, positive, positive, in...</td>\n",
       "      <td>[learn, think, positive, positive, instagram, ...</td>\n",
       "      <td>{think, instagood, learn, instagram, posit}</td>\n",
       "      <td>{think, instagood, learn, instagram, positive}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49152</th>\n",
       "      <td>49153</td>\n",
       "      <td>NaN</td>\n",
       "      <td>we love the pretty happy and fresh you teenili...</td>\n",
       "      <td>[we, love, the, pretty, happy, and, fresh, you...</td>\n",
       "      <td>[love, pretty, happy, fresh, teenilicious, fix...</td>\n",
       "      <td>{fixdermateen, generationz, love, pretti, teen...</td>\n",
       "      <td>{fixdermateen, generationz, love, fresh, happy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49153</th>\n",
       "      <td>49154</td>\n",
       "      <td>NaN</td>\n",
       "      <td>damn tuff ruff muff techno city ng web ukhx in...</td>\n",
       "      <td>[damn, tuff, ruff, muff, techno, city, ng, web...</td>\n",
       "      <td>[damn, tuff, ruff, muff, techno, city, ng, web...</td>\n",
       "      <td>{web, vk, damn, int, ruff, tuff, gabba, citi, ...</td>\n",
       "      <td>{web, city, vk, damn, int, ruff, tuff, gabba, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49154</th>\n",
       "      <td>49155</td>\n",
       "      <td>NaN</td>\n",
       "      <td>thought factory left right polarisation trump ...</td>\n",
       "      <td>[thought, factory, left, right, polarisation, ...</td>\n",
       "      <td>[thought, factory, left, right, polarisation, ...</td>\n",
       "      <td>{thought, brexit, polaris, right, leadership, ...</td>\n",
       "      <td>{politics, think, right, leadership, brexit, f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49155</th>\n",
       "      <td>49156</td>\n",
       "      <td>NaN</td>\n",
       "      <td>feeling like mermaid hairflip neverready forma...</td>\n",
       "      <td>[feeling, like, mermaid, hairflip, neverready,...</td>\n",
       "      <td>[feeling, like, mermaid, hairflip, neverready,...</td>\n",
       "      <td>{formal, mermaid, like, feel, hairflip, gown, ...</td>\n",
       "      <td>{formal, mermaid, like, feel, hairflip, neverr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49156</th>\n",
       "      <td>49157</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hillary campaigned today in ohio omg amp used ...</td>\n",
       "      <td>[hillary, campaigned, today, in, ohio, omg, am...</td>\n",
       "      <td>[hillary, campaigned, today, ohio, omg, amp, u...</td>\n",
       "      <td>{thee, today, like, clinton, asset, omg, say, ...</td>\n",
       "      <td>{thee, today, like, clinton, radicalization, o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49157</th>\n",
       "      <td>49158</td>\n",
       "      <td>NaN</td>\n",
       "      <td>happy at work conference right mindset leads t...</td>\n",
       "      <td>[happy, at, work, conference, right, mindset, ...</td>\n",
       "      <td>[happy, work, conference, right, mindset, lead...</td>\n",
       "      <td>{right, lead, mindset, cultur, organ, work, co...</td>\n",
       "      <td>{right, culture, lead, mindset, work, conferen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49158</th>\n",
       "      <td>49159</td>\n",
       "      <td>NaN</td>\n",
       "      <td>my song so glad free download shoegaze newmusi...</td>\n",
       "      <td>[my, song, so, glad, free, download, shoegaze,...</td>\n",
       "      <td>[song, glad, free, download, shoegaze, newmusi...</td>\n",
       "      <td>{newsong, shoegaz, song, download, newmus, fre...</td>\n",
       "      <td>{newsong, newmusic, song, download, free, shoe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  label                                              tweet  \\\n",
       "49149  49150    NaN  loving life createyourfuture lifestyle holiday...   \n",
       "49150  49151    NaN  black professor demonizes proposes nazi style ...   \n",
       "49151  49152    NaN  learn how to think positive positive instagram...   \n",
       "49152  49153    NaN  we love the pretty happy and fresh you teenili...   \n",
       "49153  49154    NaN  damn tuff ruff muff techno city ng web ukhx in...   \n",
       "49154  49155    NaN  thought factory left right polarisation trump ...   \n",
       "49155  49156    NaN  feeling like mermaid hairflip neverready forma...   \n",
       "49156  49157    NaN  hillary campaigned today in ohio omg amp used ...   \n",
       "49157  49158    NaN  happy at work conference right mindset leads t...   \n",
       "49158  49159    NaN  my song so glad free download shoegaze newmusi...   \n",
       "\n",
       "                                             tweet_token  \\\n",
       "49149  [loving, life, createyourfuture, lifestyle, ho...   \n",
       "49150  [black, professor, demonizes, proposes, nazi, ...   \n",
       "49151  [learn, how, to, think, positive, positive, in...   \n",
       "49152  [we, love, the, pretty, happy, and, fresh, you...   \n",
       "49153  [damn, tuff, ruff, muff, techno, city, ng, web...   \n",
       "49154  [thought, factory, left, right, polarisation, ...   \n",
       "49155  [feeling, like, mermaid, hairflip, neverready,...   \n",
       "49156  [hillary, campaigned, today, in, ohio, omg, am...   \n",
       "49157  [happy, at, work, conference, right, mindset, ...   \n",
       "49158  [my, song, so, glad, free, download, shoegaze,...   \n",
       "\n",
       "                                    tweet_token_filtered  \\\n",
       "49149  [loving, life, createyourfuture, lifestyle, ho...   \n",
       "49150  [black, professor, demonizes, proposes, nazi, ...   \n",
       "49151  [learn, think, positive, positive, instagram, ...   \n",
       "49152  [love, pretty, happy, fresh, teenilicious, fix...   \n",
       "49153  [damn, tuff, ruff, muff, techno, city, ng, web...   \n",
       "49154  [thought, factory, left, right, polarisation, ...   \n",
       "49155  [feeling, like, mermaid, hairflip, neverready,...   \n",
       "49156  [hillary, campaigned, today, ohio, omg, amp, u...   \n",
       "49157  [happy, work, conference, right, mindset, lead...   \n",
       "49158  [song, glad, free, download, shoegaze, newmusi...   \n",
       "\n",
       "                                           tweet_stemmed  \\\n",
       "49149  {life, beach, love, holiday, hyatt, lifestyl, ...   \n",
       "49150  {style, black, like, asset, germani, nazi, bre...   \n",
       "49151        {think, instagood, learn, instagram, posit}   \n",
       "49152  {fixdermateen, generationz, love, pretti, teen...   \n",
       "49153  {web, vk, damn, int, ruff, tuff, gabba, citi, ...   \n",
       "49154  {thought, brexit, polaris, right, leadership, ...   \n",
       "49155  {formal, mermaid, like, feel, hairflip, gown, ...   \n",
       "49156  {thee, today, like, clinton, asset, omg, say, ...   \n",
       "49157  {right, lead, mindset, cultur, organ, work, co...   \n",
       "49158  {newsong, shoegaz, song, download, newmus, fre...   \n",
       "\n",
       "                                        tweet_lemmatized  \n",
       "49149  {createyourfuture, life, beach, love, regency,...  \n",
       "49150  {germany, black, like, confiscation, nazi, bre...  \n",
       "49151     {think, instagood, learn, instagram, positive}  \n",
       "49152  {fixdermateen, generationz, love, fresh, happy...  \n",
       "49153  {web, city, vk, damn, int, ruff, tuff, gabba, ...  \n",
       "49154  {politics, think, right, leadership, brexit, f...  \n",
       "49155  {formal, mermaid, like, feel, hairflip, neverr...  \n",
       "49156  {thee, today, like, clinton, radicalization, o...  \n",
       "49157  {right, culture, lead, mindset, work, conferen...  \n",
       "49158  {newsong, newmusic, song, download, free, shoe...  "
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combine_df.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сохраним результат предобработки в pickle-файл."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_df.loc[combine_df['label'] != -1].to_pickle(\"df_train.pkl\")\n",
    "combine_df.loc[combine_df['label'] == -1].drop('label', axis=1).head().to_pickle(\"df_test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
